{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wIx44yo869h"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import imageio\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "# Initialize global variables.\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 5\n",
        "NUM_SAMPLES = 32\n",
        "POS_ENCODE_DIMS = 16\n",
        "EPOCHS = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy3dcUCIwjV4"
      },
      "source": [
        "### Downloading The DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "eCwN48rRK-GG",
        "outputId": "fd797de1-0d49-48bb-ffc4-670ff2767f8f"
      },
      "outputs": [],
      "source": [
        "url = \"http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\"\n",
        "file_name = \"tiny_nerf_data.npz\"\n",
        "\n",
        "if not os.path.exists(file_name):\n",
        "    data = keras.utils.get_file(fname=file_name, origin=url)\n",
        "\n",
        "data = np.load(data)\n",
        "im_shape = data[\"images\"].shape\n",
        "(num_images, H, W, _) = data[\"images\"].shape\n",
        "\n",
        "poses = data[\"poses\"]\n",
        "focal =  data[\"focal\"]\n",
        "print(\"Total number of Images :\",num_images)\n",
        "print(\"focal length between Cam and Object \",focal)\n",
        "print(\"Image Shape: \",H,W)\n",
        "\n",
        "# Plot a random image from the dataset for visualization.\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(data[\"images\"][np.random.randint(low=0, high=num_images)])\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTHJjZzwzqtL"
      },
      "source": [
        "## Data pipeline\n",
        "\n",
        "Now that you've understood the notion of camera matrix\n",
        "and the mapping from a 3D scene to 2D images,\n",
        "let's talk about the inverse mapping, i.e. from 2D image to the 3D scene.\n",
        "\n",
        "We'll need to talk about volumetric rendering with ray casting and tracing,\n",
        "which are common computer graphics techniques.\n",
        "This section will help you get to speed with these techniques.\n",
        "\n",
        "Consider an image with `N` pixels. We shoot a ray through each pixel\n",
        "and sample some points on the ray. A ray is commonly parameterized by\n",
        "the equation `r(t) = o + td` where `t` is the parameter, `o` is the\n",
        "origin and `d` is the unit directional vector as shown in **Figure 6**.\n",
        "\n",
        "| ![img](https://i.imgur.com/ywrqlzt.gif) |\n",
        "| :---: |\n",
        "| **Figure 6**: `r(t) = o + td` where t is 3 |\n",
        "\n",
        "In **Figure 7**, we consider a ray, and we sample some random points on\n",
        "the ray. These sample points each have a unique location `(x, y, z)`\n",
        "and the ray has a viewing angle `(theta, phi)`. The viewing angle is\n",
        "particularly interesting as we can shoot a ray through a single pixel\n",
        "in a lot of different ways, each with a unique viewing angle. Another\n",
        "interesting thing to notice here is the noise that is added to the\n",
        "sampling process. We add a uniform noise to each sample so that the\n",
        "samples correspond to a continuous distribution. In **Figure 7** the\n",
        "blue points are the evenly distributed samples and the white points\n",
        "`(t1, t2, t3)` are randomly placed between the samples.\n",
        "\n",
        "| ![img](https://i.imgur.com/r9TS2wv.gif) |\n",
        "| :---: |\n",
        "| **Figure 7**: Sampling the points from a ray. |\n",
        "\n",
        "**Figure 8** showcases the entire sampling process in 3D, where you\n",
        "can see the rays coming out of the white image. This means that each\n",
        "pixel will have its corresponding rays and each ray will be sampled at\n",
        "distinct points.\n",
        "\n",
        "| ![3-d rays](https://i.imgur.com/hr4D2g2.gif) |\n",
        "| :---: |\n",
        "| **Figure 8**: Shooting rays from all the pixels of an image in 3-D |\n",
        "\n",
        "These sampled points act as the input to the NeRF model. The model is\n",
        "then asked to predict the RGB color and the volume density at that\n",
        "point.\n",
        "\n",
        "| ![3-Drender](https://i.imgur.com/HHb6tlQ.png) |\n",
        "| :---: |\n",
        "| **Figure 9**: Data pipeline <br>\n",
        "[Source: NeRF](https://arxiv.org/abs/2003.08934) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buc69OYnzrIM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def encode_position(x):\n",
        "    \"\"\"Encodes the position into its corresponding Fourier feature.\n",
        "\n",
        "    Args:\n",
        "        x: The input coordinate.\n",
        "\n",
        "    Returns:\n",
        "        Fourier features tensors of the position.\n",
        "    \"\"\"\n",
        "    positions = [x]\n",
        "    for i in range(POS_ENCODE_DIMS):\n",
        "        for fn in [tf.sin, tf.cos]:\n",
        "            positions.append(fn(2.0 ** i * x))\n",
        "    return tf.concat(positions, axis=-1)\n",
        "\n",
        "\n",
        "def get_rays(height, width, focal, pose):\n",
        "    \"\"\"Computes origin point and direction vector of rays.\n",
        "\n",
        "    Args:\n",
        "        height: Height of the image.\n",
        "        width: Width of the image.\n",
        "        focal: The focal length between the images and the camera.\n",
        "        pose: The pose matrix of the camera.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of origin point and direction vector for rays.\n",
        "    \"\"\"\n",
        "    # Build a meshgrid for the rays.\n",
        "    i, j = tf.meshgrid(\n",
        "        tf.range(width, dtype=tf.float32),\n",
        "        tf.range(height, dtype=tf.float32),\n",
        "        indexing=\"xy\",\n",
        "    )\n",
        "\n",
        "    # Normalize the x axis coordinates.\n",
        "    transformed_i = (i - width * 0.5) / focal\n",
        "\n",
        "    # Normalize the y axis coordinates.\n",
        "    transformed_j = (j - height * 0.5) / focal\n",
        "\n",
        "    # Create the direction unit vectors.\n",
        "    directions = tf.stack([transformed_i, -transformed_j, -tf.ones_like(i)], axis=-1)\n",
        "\n",
        "    # Get the camera matrix.\n",
        "    camera_matrix = pose[:3, :3]\n",
        "    height_width_focal = pose[:3, -1]\n",
        "\n",
        "    # Get origins and directions for the rays.\n",
        "    transformed_dirs = directions[..., None, :]\n",
        "    camera_dirs = transformed_dirs * camera_matrix\n",
        "    ray_directions = tf.reduce_sum(camera_dirs, axis=-1)\n",
        "    ray_origins = tf.broadcast_to(height_width_focal, tf.shape(ray_directions))\n",
        "\n",
        "    # Return the origins and directions.\n",
        "    return (ray_origins, ray_directions)\n",
        "\n",
        "\n",
        "def render_flat_rays(ray_origins, ray_directions, near, far, num_samples, rand=False):\n",
        "    \"\"\"Renders the rays and flattens it.\n",
        "\n",
        "    Args:\n",
        "        ray_origins: The origin points for rays.\n",
        "        ray_directions: The direction unit vectors for the rays.\n",
        "        near: The near bound of the volumetric scene.\n",
        "        far: The far bound of the volumetric scene.\n",
        "        num_samples: Number of sample points in a ray.\n",
        "        rand: Choice for randomising the sampling strategy.\n",
        "\n",
        "    Returns:\n",
        "       Tuple of flattened rays and sample points on each rays.\n",
        "    \"\"\"\n",
        "    # Compute 3D query points.\n",
        "    # Equation: r(t) = o+td -> Building the \"t\" here.\n",
        "    t_vals = tf.linspace(near, far, num_samples)\n",
        "    if rand:\n",
        "        # Inject uniform noise into sample space to make the sampling\n",
        "        # continuous.\n",
        "        shape = list(ray_origins.shape[:-1]) + [num_samples]\n",
        "        noise = tf.random.uniform(shape=shape) * (far - near) / num_samples\n",
        "        t_vals = t_vals + noise\n",
        "\n",
        "    # Equation: r(t) = o + td -> Building the \"r\" here.\n",
        "    rays = ray_origins[..., None, :] + (\n",
        "        ray_directions[..., None, :] * t_vals[..., None]\n",
        "    )\n",
        "    rays_flat = tf.reshape(rays, [-1, 3])\n",
        "    rays_flat = encode_position(rays_flat)\n",
        "    return (rays_flat, t_vals)\n",
        "\n",
        "\n",
        "def map_fn(pose):\n",
        "    \"\"\"Maps individual pose to flattened rays and sample points.\n",
        "\n",
        "    Args:\n",
        "        pose: The pose matrix of the camera.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of flattened rays and sample points corresponding to the\n",
        "        camera pose.\n",
        "    \"\"\"\n",
        "    (ray_origins, ray_directions) = get_rays(height=H, width=W, focal=focal, pose=pose)\n",
        "    (rays_flat, t_vals) = render_flat_rays(\n",
        "        ray_origins=ray_origins,\n",
        "        ray_directions=ray_directions,\n",
        "        near=2.0,\n",
        "        far=6.0,\n",
        "        num_samples=NUM_SAMPLES,\n",
        "        rand=True,\n",
        "    )\n",
        "    return (rays_flat, t_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QfRkNu1z-uh",
        "outputId": "87c0471e-124b-4094-de9e-46adb1a7f9c0"
      },
      "outputs": [],
      "source": [
        "# Create the training split.\n",
        "split_index = int(num_images * 0.8)\n",
        "\n",
        "# Split the images into training and validation.\n",
        "train_images = data[\"images\"][:split_index]\n",
        "val_images = data['images'][split_index:]\n",
        "\n",
        "# Split the poses into training and validation.\n",
        "train_poses = poses[:split_index]\n",
        "val_poses = poses[split_index:]\n",
        "\n",
        "print(\"Train Images:\",train_images.shape)\n",
        "print(\"Validation Images :\",val_images.shape)\n",
        "\n",
        "print(\"Train Poses :\",train_poses.shape)\n",
        "print(\"val Poses :\",val_poses.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-9.9990219e-01,  4.1922452e-03, -1.3345719e-02, -5.3798322e-02],\n",
              "       [-1.3988681e-02, -2.9965907e-01,  9.5394367e-01,  3.8454704e+00],\n",
              "       [-4.6566129e-10,  9.5403719e-01,  2.9968831e-01,  1.2080823e+00],\n",
              "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.0000000e+00]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wOeYug5IcbU"
      },
      "outputs": [],
      "source": [
        "del(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMKBPt2Hz8yk"
      },
      "outputs": [],
      "source": [
        "# Make the training pipeline.\n",
        "train_img_ds = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "train_pose_ds = tf.data.Dataset.from_tensor_slices(train_poses)\n",
        "\n",
        "train_ray_ds = train_pose_ds.map(map_fn, num_parallel_calls=AUTO)\n",
        "training_ds = tf.data.Dataset.zip((train_img_ds, train_ray_ds))\n",
        "\n",
        "train_ds = (\n",
        "    training_ds.shuffle(BATCH_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "# Make the validation pipeline.\n",
        "val_img_ds = tf.data.Dataset.from_tensor_slices(val_images)\n",
        "val_pose_ds = tf.data.Dataset.from_tensor_slices(val_poses)\n",
        "val_ray_ds = val_pose_ds.map(map_fn, num_parallel_calls=AUTO)\n",
        "validation_ds = tf.data.Dataset.zip((val_img_ds, val_ray_ds))\n",
        "val_ds = (\n",
        "    validation_ds.shuffle(BATCH_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOqKPA8F0d7b"
      },
      "source": [
        "## NeRF model\n",
        "\n",
        "The model is a multi-layer perceptron (MLP), with ReLU as its non-linearity.\n",
        "\n",
        "An excerpt from the paper:\n",
        "\n",
        "*\"We encourage the representation to be multiview-consistent by\n",
        "restricting the network to predict the volume density sigma as a\n",
        "function of only the location `x`, while allowing the RGB color `c` to be\n",
        "predicted as a function of both location and viewing direction. To\n",
        "accomplish this, the MLP first processes the input 3D coordinate `x`\n",
        "with 8 fully-connected layers (using ReLU activations and 256 channels\n",
        "per layer), and outputs sigma and a 256-dimensional feature vector.\n",
        "This feature vector is then concatenated with the camera ray's viewing\n",
        "direction and passed to one additional fully-connected layer (using a\n",
        "ReLU activation and 128 channels) that output the view-dependent RGB\n",
        "color.\"*\n",
        "\n",
        "Here we have gone for a minimal implementation and have used 64\n",
        "Dense units instead of 256 as mentioned in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvqnGzKt0Z4W"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_nerf_model(num_layers, num_pos):\n",
        "    \"\"\"Generates the NeRF neural network.\n",
        "\n",
        "    Args:\n",
        "        num_layers: The number of MLP layers.\n",
        "        num_pos: The number of dimensions of positional encoding.\n",
        "\n",
        "    Returns:\n",
        "        The `tf.keras` model.\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(num_pos, 2 * 3 * POS_ENCODE_DIMS + 3))\n",
        "    x = inputs\n",
        "    for i in range(num_layers):\n",
        "        x = layers.Dense(units=64, activation=\"relu\")(x)\n",
        "        if i % 4 == 0 and i > 0:\n",
        "            # Inject residual connection.\n",
        "            x = layers.concatenate([x, inputs], axis=-1)\n",
        "    outputs = layers.Dense(units=4)(x)\n",
        "    return keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "def render_rgb_depth(model, rays_flat, t_vals, rand=True, train=True):\n",
        "    \"\"\"Generates the RGB image and depth map from model prediction.\n",
        "\n",
        "    Args:\n",
        "        model: The MLP model that is trained to predict the rgb and\n",
        "            volume density of the volumetric scene.\n",
        "        rays_flat: The flattened rays that serve as the input to\n",
        "            the NeRF model.\n",
        "        t_vals: The sample points for the rays.\n",
        "        rand: Choice to randomise the sampling strategy.\n",
        "        train: Whether the model is in the training or testing phase.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of rgb image and depth map.\n",
        "    \"\"\"\n",
        "    # Get the predictions from the nerf model and reshape it.\n",
        "    if train:\n",
        "        predictions = model(rays_flat)\n",
        "    else:\n",
        "        predictions = model.predict(rays_flat)\n",
        "    predictions = tf.reshape(predictions, shape=(BATCH_SIZE, H, W, NUM_SAMPLES, 4))\n",
        "\n",
        "    # Slice the predictions into rgb and sigma.\n",
        "    rgb = tf.sigmoid(predictions[..., :-1])\n",
        "    sigma_a = tf.nn.relu(predictions[..., -1])\n",
        "\n",
        "    # Get the distance of adjacent intervals.\n",
        "    delta = t_vals[..., 1:] - t_vals[..., :-1]\n",
        "    # delta shape = (num_samples)\n",
        "    if rand:\n",
        "        delta = tf.concat(\n",
        "            [delta, tf.broadcast_to([1e10], shape=(BATCH_SIZE, H, W, 1))], axis=-1\n",
        "        )\n",
        "        alpha = 1.0 - tf.exp(-sigma_a * delta)\n",
        "    else:\n",
        "        delta = tf.concat(\n",
        "            [delta, tf.broadcast_to([1e10], shape=(BATCH_SIZE, 1))], axis=-1\n",
        "        )\n",
        "        alpha = 1.0 - tf.exp(-sigma_a * delta[:, None, None, :])\n",
        "\n",
        "    # Get transmittance.\n",
        "    exp_term = 1.0 - alpha\n",
        "    epsilon = 1e-10\n",
        "    transmittance = tf.math.cumprod(exp_term + epsilon, axis=-1, exclusive=True)\n",
        "    weights = alpha * transmittance\n",
        "    rgb = tf.reduce_sum(weights[..., None] * rgb, axis=-2)\n",
        "\n",
        "    if rand:\n",
        "        depth_map = tf.reduce_sum(weights * t_vals, axis=-1)\n",
        "    else:\n",
        "        depth_map = tf.reduce_sum(weights * t_vals[:, None, None], axis=-1)\n",
        "    return (rgb, depth_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYII-lGA0uF_"
      },
      "source": [
        "## Training\n",
        "\n",
        "The training step is implemented as part of a custom `keras.Model` subclass\n",
        "so that we can make use of the `model.fit` functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLhHGWNT0sH6"
      },
      "outputs": [],
      "source": [
        "class NeRF(keras.Model):\n",
        "    def __init__(self, nerf_model):\n",
        "        super().__init__()\n",
        "        self.nerf_model = nerf_model\n",
        "\n",
        "    def compile(self, optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.psnr_metric = keras.metrics.Mean(name=\"psnr\")\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        # Get the images and the rays.\n",
        "        (images, rays) = inputs\n",
        "        (rays_flat, t_vals) = rays\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Get the predictions from the model.\n",
        "            rgb, _ = render_rgb_depth(\n",
        "                model=self.nerf_model, rays_flat=rays_flat, t_vals=t_vals, rand=True\n",
        "            )\n",
        "            loss = self.loss_fn(images, rgb)\n",
        "\n",
        "        # Get the trainable variables.\n",
        "        trainable_variables = self.nerf_model.trainable_variables\n",
        "\n",
        "        # Get the gradeints of the trainiable variables with respect to the loss.\n",
        "        gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "        # Apply the grads and optimize the model.\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "        # Get the PSNR of the reconstructed images and the source images.\n",
        "        psnr = tf.image.psnr(images, rgb, max_val=1.0)\n",
        "\n",
        "        # Compute our own metrics\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.psnr_metric.update_state(psnr)\n",
        "        return {\"loss\": self.loss_tracker.result(), \"psnr\": self.psnr_metric.result()}\n",
        "\n",
        "    def test_step(self, inputs):\n",
        "        # Get the images and the rays.\n",
        "        (images, rays) = inputs\n",
        "        (rays_flat, t_vals) = rays\n",
        "\n",
        "        # Get the predictions from the model.\n",
        "        rgb, _ = render_rgb_depth(\n",
        "            model=self.nerf_model, rays_flat=rays_flat, t_vals=t_vals, rand=True\n",
        "        )\n",
        "        loss = self.loss_fn(images, rgb)\n",
        "\n",
        "        # Get the PSNR of the reconstructed images and the source images.\n",
        "        psnr = tf.image.psnr(images, rgb, max_val=1.0)\n",
        "\n",
        "        # Compute our own metrics\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.psnr_metric.update_state(psnr)\n",
        "        return {\"loss\": self.loss_tracker.result(), \"psnr\": self.psnr_metric.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.psnr_metric]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEkuxPKY9sZ1"
      },
      "outputs": [],
      "source": [
        "test_imgs, test_rays = next(iter(train_ds))\n",
        "test_rays_flat, test_t_vals = test_rays\n",
        "\n",
        "loss_list = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr7rblLF4Ls_"
      },
      "source": [
        "### Nerf Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jLR9t_q4PVH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "class TrainMonitor(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch+1) %10 == 0:\n",
        "            loss = logs[\"loss\"]\n",
        "            loss_list.append(loss)\n",
        "            test_recons_images, depth_maps = render_rgb_depth(\n",
        "                model=self.model.nerf_model,\n",
        "                rays_flat=test_rays_flat,\n",
        "                t_vals=test_t_vals,\n",
        "                rand=True,\n",
        "                train=False,\n",
        "            )\n",
        "\n",
        "            # Plot the rgb, depth and the loss plot.\n",
        "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
        "            ax[0].imshow(keras.utils.array_to_img(test_recons_images[0]))\n",
        "            ax[0].set_title(f\"Predicted Image: {epoch+1:03d}\")\n",
        "\n",
        "            ax[1].imshow(keras.utils.array_to_img(depth_maps[0, ..., None]))\n",
        "            ax[1].set_title(f\"Depth Map: {epoch+1:03d}\")\n",
        "\n",
        "            # fig.savefig(f\"images/{epoch+1:03d}.png\")\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "checkpoint_filepath = '/content/nerf/checkpoint'\n",
        "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                save_weights_only=True,monitor='val_psnr',mode='max',save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfCUcZ4Q03Bs"
      },
      "outputs": [],
      "source": [
        "num_pos = H * W * NUM_SAMPLES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBhoCJbH9z1k"
      },
      "source": [
        "### Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB_C1H9z9zV8"
      },
      "outputs": [],
      "source": [
        "nerf_model = get_nerf_model(num_layers=8, num_pos=num_pos)\n",
        "model = NeRF(nerf_model)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(), loss_fn=keras.losses.MeanSquaredError()\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[TrainMonitor(),model_checkpoint_callback],\n",
        "    steps_per_epoch=split_index // BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3gw1IEm4_Rk"
      },
      "source": [
        "#### Inference\n",
        "\n",
        "In this section, we ask the model to build novel views of the scene.\n",
        "The model was given `106` views of the scene in the training step. The\n",
        "collections of training images cannot contain each and every angle of\n",
        "the scene. A trained model can represent the entire 3-D scene with a\n",
        "sparse set of training images.\n",
        "\n",
        "Here we provide different poses to the model and ask for it to give us\n",
        "the 2-D image corresponding to that camera view. If we infer the model\n",
        "for all the 360-degree views, it should provide an overview of the\n",
        "entire scenery from all around."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMpVYSzL8fgU",
        "outputId": "6d20042b-c8f1-44d0-f8d4-a5d4cc0ac115"
      },
      "outputs": [],
      "source": [
        "nerf_model = get_nerf_model(num_layers=8, num_pos=num_pos)\n",
        "model = NeRF(nerf_model)\n",
        "model.load_weights(checkpoint_filepath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aKmcr1bq7PGr",
        "outputId": "5d8d1ec0-4dba-4ef1-a733-929389244a10"
      },
      "outputs": [],
      "source": [
        "# Get the trained NeRF model and infer.\n",
        "nerf_model = model.nerf_model\n",
        "test_recons_images, depth_maps = render_rgb_depth(\n",
        "    model=nerf_model,\n",
        "    rays_flat=test_rays_flat,\n",
        "    t_vals=test_t_vals,\n",
        "    rand=True,\n",
        "    train=False,\n",
        ")\n",
        "\n",
        "# Create subplots.\n",
        "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(10, 20))\n",
        "\n",
        "for ax, ori_img, recons_img, depth_map in zip(\n",
        "    axes, test_imgs, test_recons_images, depth_maps\n",
        "):\n",
        "    ax[0].imshow(keras.utils.array_to_img(ori_img))\n",
        "    ax[0].set_title(\"Original\")\n",
        "\n",
        "    ax[1].imshow(keras.utils.array_to_img(recons_img))\n",
        "    ax[1].set_title(\"Reconstructed\")\n",
        "\n",
        "    ax[2].imshow(\n",
        "        keras.utils.array_to_img(depth_map[..., None]), cmap=\"inferno\"\n",
        "    )\n",
        "    ax[2].set_title(\"Depth Map\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upo2VxnU7egi"
      },
      "source": [
        "### Render 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io13IQXO5BQo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_translation_t(t):\n",
        "    \"\"\"Get the translation matrix for movement in t.\"\"\"\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [0, 0, 1, t],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def get_rotation_phi(phi):\n",
        "    \"\"\"Get the rotation matrix for movement in phi.\"\"\"\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, tf.cos(phi), -tf.sin(phi), 0],\n",
        "        [0, tf.sin(phi), tf.cos(phi), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def get_rotation_theta(theta):\n",
        "    \"\"\"Get the rotation matrix for movement in theta.\"\"\"\n",
        "    matrix = [\n",
        "        [tf.cos(theta), 0, -tf.sin(theta), 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [tf.sin(theta), 0, tf.cos(theta), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, t):\n",
        "    \"\"\"\n",
        "    Get the camera to world matrix for the corresponding theta, phi\n",
        "    and t.\n",
        "    \"\"\"\n",
        "    c2w = get_translation_t(t)\n",
        "    c2w = get_rotation_phi(phi / 180.0 * np.pi) @ c2w\n",
        "    c2w = get_rotation_theta(theta / 180.0 * np.pi) @ c2w\n",
        "    c2w = np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]) @ c2w\n",
        "    return c2w\n",
        "\n",
        "\n",
        "rgb_frames = []\n",
        "batch_flat = []\n",
        "batch_t = []\n",
        "\n",
        "# Iterate over different theta value and generate scenes.\n",
        "for index, theta in tqdm(enumerate(np.linspace(0.0, 360.0, 120, endpoint=False))):\n",
        "    # Get the camera to world matrix.\n",
        "    c2w = pose_spherical(theta, -30.0, 4.0)\n",
        "    ray_oris, ray_dirs = get_rays(H, W, focal, c2w)\n",
        "    rays_flat, t_vals = render_flat_rays(\n",
        "        ray_oris, ray_dirs, near=2.0, far=6.0, num_samples=NUM_SAMPLES, rand=False\n",
        "    )\n",
        "\n",
        "    if index % BATCH_SIZE == 0 and index > 0:\n",
        "        batched_flat = tf.stack(batch_flat, axis=0)\n",
        "        batch_flat = [rays_flat]\n",
        "\n",
        "        batched_t = tf.stack(batch_t, axis=0)\n",
        "        batch_t = [t_vals]\n",
        "\n",
        "        rgb, _ = render_rgb_depth(\n",
        "            nerf_model, batched_flat, batched_t, rand=False, train=False\n",
        "        )\n",
        "\n",
        "        temp_rgb = [np.clip(255 * img, 0.0, 255.0).astype(np.uint8) for img in rgb]\n",
        "\n",
        "        rgb_frames = rgb_frames + temp_rgb\n",
        "    else:\n",
        "        batch_flat.append(rays_flat)\n",
        "        batch_t.append(t_vals)\n",
        "\n",
        "rgb_video = \"rgb_video.mp4\"\n",
        "imageio.mimwrite(rgb_video, rgb_frames, fps=30, quality=7, macro_block_size=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObLTiuIPDq-x"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_translation_t(t):\n",
        "    \"\"\"Get the translation matrix for movement in t.\"\"\"\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [0, 0, 1, t],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def get_rotation_phi(phi):\n",
        "    \"\"\"Get the rotation matrix for movement in phi.\"\"\"\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, tf.cos(phi), -tf.sin(phi), 0],\n",
        "        [0, tf.sin(phi), tf.cos(phi), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def get_rotation_theta(theta):\n",
        "    \"\"\"Get the rotation matrix for movement in theta.\"\"\"\n",
        "    matrix = [\n",
        "        [tf.cos(theta), 0, -tf.sin(theta), 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [tf.sin(theta), 0, tf.cos(theta), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, t):\n",
        "    \"\"\"\n",
        "    Get the camera to world matrix for the corresponding theta, phi\n",
        "    and t.\n",
        "    \"\"\"\n",
        "    c2w = get_translation_t(t)\n",
        "    c2w = get_rotation_phi(phi / 180.0 * np.pi) @ c2w\n",
        "    c2w = get_rotation_theta(theta / 180.0 * np.pi) @ c2w\n",
        "    c2w = np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]) @ c2w\n",
        "    return c2w\n",
        "\n",
        "\n",
        "rgb_frames = []\n",
        "batch_flat = []\n",
        "batch_t = []\n",
        "\n",
        "# Iterate over different theta value and generate scenes.\n",
        "for index, theta in tqdm(enumerate(np.linspace(0.0, 360.0, 120, endpoint=False))):\n",
        "    # Get the camera to world matrix.\n",
        "    c2w = pose_spherical(theta, -30.0, 4.0)\n",
        "    ray_oris, ray_dirs = get_rays(H, W, focal, c2w)\n",
        "    rays_flat, t_vals = render_flat_rays(\n",
        "        ray_oris, ray_dirs, near=2.0, far=6.0, num_samples=NUM_SAMPLES, rand=False\n",
        "    )\n",
        "\n",
        "    if index % BATCH_SIZE == 0 and index > 0:\n",
        "        batched_flat = tf.stack(batch_flat, axis=0)\n",
        "        batch_flat = [rays_flat]\n",
        "\n",
        "        batched_t = tf.stack(batch_t, axis=0)\n",
        "        batch_t = [t_vals]\n",
        "\n",
        "        rgb, _ = render_rgb_depth(\n",
        "            nerf_model, batched_flat, batched_t, rand=False, train=False\n",
        "        )\n",
        "\n",
        "        temp_rgb = [np.clip(255 * img, 0.0, 255.0).astype(np.uint8) for img in rgb]\n",
        "\n",
        "        rgb_frames = rgb_frames + temp_rgb\n",
        "    else:\n",
        "        batch_flat.append(rays_flat)\n",
        "        batch_t.append(t_vals)\n",
        "\n",
        "rgb_video = \"rgb_video.mp4\"\n",
        "imageio.mimwrite(rgb_video, rgb_frames, fps=30, quality=7, macro_block_size=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "FApc1k355EHk",
        "outputId": "b51d7a38-c007-432c-d1e9-cbb328976cc5"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('rgb_video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls autoplay loop>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zctBOgfbEGPc",
        "outputId": "7ce8a236-6709-4e51-d558-0b0159a19266"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_translation_t(t):\n",
        "    \"\"\"Get the translation matrix for movement in t.\"\"\"\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [0, 0, 1, t],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def get_rotation_phi(phi):\n",
        "    \"\"\"Get the rotation matrix for movement in phi.\"\"\"\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, tf.cos(phi), -tf.sin(phi), 0],\n",
        "        [0, tf.sin(phi), tf.cos(phi), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def get_rotation_theta(theta):\n",
        "    \"\"\"Get the rotation matrix for movement in theta.\"\"\"\n",
        "    matrix = [\n",
        "        [tf.cos(theta), 0, -tf.sin(theta), 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [tf.sin(theta), 0, tf.cos(theta), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, t):\n",
        "    \"\"\"\n",
        "    Get the camera to world matrix for the corresponding theta, phi\n",
        "    and t.\n",
        "    \"\"\"\n",
        "    c2w = get_translation_t(t)\n",
        "    c2w = get_rotation_phi(phi / 180.0 * np.pi) @ c2w\n",
        "    c2w = get_rotation_theta(theta / 180.0 * np.pi) @ c2w\n",
        "    c2w = np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]) @ c2w\n",
        "    return c2w\n",
        "\n",
        "\n",
        "depth_frames = []\n",
        "batch_flat = []\n",
        "batch_t = []\n",
        "\n",
        "# Iterate over different theta value and generate scenes.\n",
        "for index, theta in tqdm(enumerate(np.linspace(0.0, 360.0, 120, endpoint=False))):\n",
        "    # Get the camera to world matrix.\n",
        "    c2w = pose_spherical(theta, -30.0, 4.0)\n",
        "    ray_oris, ray_dirs = get_rays(H, W, focal, c2w)\n",
        "    rays_flat, t_vals = render_flat_rays(\n",
        "        ray_oris, ray_dirs, near=2.0, far=6.0, num_samples=NUM_SAMPLES, rand=False\n",
        "    )\n",
        "\n",
        "    if index % BATCH_SIZE == 0 and index > 0:\n",
        "        batched_flat = tf.stack(batch_flat, axis=0)\n",
        "        batch_flat = [rays_flat]\n",
        "\n",
        "        batched_t = tf.stack(batch_t, axis=0)\n",
        "        batch_t = [t_vals]\n",
        "\n",
        "        _, depth = render_rgb_depth(\n",
        "            nerf_model, batched_flat, batched_t, rand=False, train=False\n",
        "        )\n",
        "\n",
        "        temp_depth = [np.clip(255 * img, 0.0, 255.0).astype(np.uint8) for img in depth]\n",
        "\n",
        "        depth_frames = depth_frames + temp_depth\n",
        "    else:\n",
        "        batch_flat.append(rays_flat)\n",
        "        batch_t.append(t_vals)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obRLO17aErOg"
      },
      "outputs": [],
      "source": [
        "depth_video = \"depth_video.mp4\"\n",
        "imageio.mimwrite(depth_video, depth_frames, fps=30, quality=7, macro_block_size=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "EgEwMQybFbiW",
        "outputId": "393d694d-ddfb-4828-c0c6-e7d514eb8341"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('depth_video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls autoplay loop>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
