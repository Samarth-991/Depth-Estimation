{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Cite from: https://github.com/simonmeister/pytorch-mono-depth\n",
    "import os \n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from math import log\n",
    "from matplotlib import pyplot as plt \n",
    "import sys \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "sys.path.append(\"../src\")\n",
    "from depth_model.model import PTModel\n",
    "from data_loader.data_creation import CreateDataset\n",
    "from utils.data_transforms import pre_process\n",
    "\n",
    "print(th.cuda.is_available())\n",
    "device = 'cuda' if th.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Losses \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mask_input(input, mask=None):\n",
    "    if mask is not None:\n",
    "        input = input * mask\n",
    "        count = th.sum(mask).data[0]\n",
    "    else:\n",
    "        count = np.prod(input.size(), dtype=np.float32).item()\n",
    "    return input, count\n",
    "\n",
    "class BerHuLoss(nn.Module):\n",
    "    def forward(self, input, target, mask=None):\n",
    "        x = input - target\n",
    "        abs_x = th.abs(x)\n",
    "        c = th.max(abs_x).item() / 5\n",
    "        leq = (abs_x <= c).float()\n",
    "        l2_losses = (x ** 2 + c ** 2) / (2 * c)\n",
    "        losses = leq * abs_x + (1 - leq) * l2_losses\n",
    "        losses, count = _mask_input(losses, mask)\n",
    "        return th.sum(losses) / count\n",
    "\n",
    "class HuberLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.SmoothL1Loss(size_average=False)\n",
    "\n",
    "    def forward(self, input, target, mask=None):\n",
    "        if mask is not None:\n",
    "            loss = self.loss(input * mask, target * mask)\n",
    "            count = th.sum(mask).data[0]\n",
    "            return loss / count\n",
    "\n",
    "        count = np.prod(input.size(), dtype=np.float32).item()\n",
    "        return self.loss(input, target) / count\n",
    "\n",
    "class DistributionLogLoss(nn.Module):\n",
    "    def __init__(self, distribution):\n",
    "        super().__init__()\n",
    "        self.distribution = distribution\n",
    "\n",
    "    def forward(self, input, target, mask=None):\n",
    "        d = self.distribution(*input)\n",
    "        loss = d.log_loss(target)\n",
    "        loss, count = _mask_input(loss, mask)\n",
    "        return th.sum(loss) / count\n",
    "    \n",
    "\n",
    "class RMSLoss(nn.Module):\n",
    "    def forward(self, input, target, mask=None):\n",
    "        loss = th.pow(input - target, 2)\n",
    "        loss, count = _mask_input(loss, mask)\n",
    "        return th.sqrt(th.sum(loss) / count)\n",
    "    \n",
    "\n",
    "class RelLoss(nn.Module):\n",
    "    def forward(self, input, target, mask=None):\n",
    "        loss = th.abs(input - target) / target\n",
    "        loss, count = _mask_input(loss, mask)\n",
    "        return th.sum(loss) / count\n",
    "    \n",
    "class MseLoss(nn.Module):  \n",
    "    def forward(self, input, target, mask=None):\n",
    "        loss = th.sum((input - target) ** 2)\n",
    "        loss, count = _mask_input(loss, mask)\n",
    "        return th.sum(loss) / count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samartht/anaconda3/envs/pydepth/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/samartht/anaconda3/envs/pydepth/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_model = PTModel().to(device)\n",
    "depth_model.load_state_dict(th.load(\"../model/child_depth_model.ckpt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference and get metric on Validation Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion  = BerHuLoss()\n",
    "criterion2 = RelLoss() \n",
    "criterion3 = RMSLoss()\n",
    "\n",
    "def validate(model, test_loader,batch_size = 1):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "      loss2, loss ,loss3 = 0.0, 0.0, 0.0\n",
    "      for sample_batch in tqdm(test_loader,total=len(test_loader)):\n",
    "            t_image = sample_batch['image']\n",
    "            t_depth = sample_batch['depth']\n",
    "            \n",
    "            t_image = t_image.to(device)\n",
    "            t_depth = t_depth.to(device)\n",
    "            t_outputs = model(t_image)\n",
    "            \n",
    "            curr_loss = criterion(t_depth, t_outputs) ## BerHuLoss\n",
    "            curr_loss2 = criterion2(t_depth, t_outputs) ## REL Loss\n",
    "            curr_loss3 = criterion3(t_depth,t_outputs) ## RMS Loss\n",
    "            loss += curr_loss.item()\n",
    "            loss2 += curr_loss2.item()\n",
    "            loss3 += curr_loss3.item()\n",
    "            \n",
    "      print(\"Validation BerHuLoss: {:.4f}\"\n",
    "            .format(loss/(len(test_loader) * batch_size)))\n",
    "      print(\"Validation REL LOSS: {:.4f}\"\n",
    "            .format(loss2/(len(test_loader) * batch_size)))\n",
    "      print(\"Validation RMSE: {:.4f}\"\n",
    "            .format(loss3/(len(test_loader) * batch_size)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation on Assignment data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from glob import glob \n",
    "import sys \n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from  utils.load_tof_images import create_from_zip_absolute  as load_assignment_data\n",
    "from data_loader.data_loader_assignment import CreateAssignemntDataset\n",
    "from utils.data_transforms import pre_process\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RGB images  255\n",
      "Total Depth Images  255\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/360_scan/\"\n",
    "child_rgb_files = glob(path+\"/*/rgb/*\")\n",
    "child_depth_files = glob(path+'/*/depth/*')\n",
    "\n",
    "print(\"Total RGB images \",len(child_rgb_files))\n",
    "print(\"Total Depth Images \",len(child_depth_files))\n",
    "\n",
    "batch_size = 1\n",
    "assignment_loader = CreateAssignemntDataset(child_rgb_files,child_depth_files,process_image=pre_process())\n",
    "valid_loader = DataLoader(assignment_loader, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/255 [00:00<?, ?it/s]/mnt/e/Personal/Samarth/repository/Depth-Estimation/notebooks/../src/utils/data_transforms.py:43: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  img = th.from_numpy(np.transpose(pic, (2, 0, 1)))\n",
      "100%|██████████| 255/255 [01:36<00:00,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BerHuLoss: 0.2191\n",
      "Validation REL LOSS: 0.0208\n",
      "Validation RMSE: 0.4057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validate(depth_model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
