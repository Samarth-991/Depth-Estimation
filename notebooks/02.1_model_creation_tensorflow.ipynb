{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "tf.random.set_seed(12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/val/val/indoors/\"\n",
    "\n",
    "filelist = []\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        filelist.append(os.path.join(root, file))\n",
    "\n",
    "filelist.sort()\n",
    "data = {\n",
    "    \"image\": [x for x in filelist if x.endswith(\".png\")],\n",
    "    \"depth\": [x for x in filelist if x.endswith(\"_depth.npy\")],\n",
    "    \"mask\":  [x for x in filelist if x.endswith(\"_depth_mask.npy\")],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>depth</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>../data/val/val/indoors/scene_00021/scan_00188...</td>\n",
       "      <td>../data/val/val/indoors/scene_00021/scan_00188...</td>\n",
       "      <td>../data/val/val/indoors/scene_00021/scan_00188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>../data/val/val/indoors/scene_00020/scan_00185...</td>\n",
       "      <td>../data/val/val/indoors/scene_00020/scan_00185...</td>\n",
       "      <td>../data/val/val/indoors/scene_00020/scan_00185...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>../data/val/val/indoors/scene_00021/scan_00189...</td>\n",
       "      <td>../data/val/val/indoors/scene_00021/scan_00189...</td>\n",
       "      <td>../data/val/val/indoors/scene_00021/scan_00189...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>../data/val/val/indoors/scene_00019/scan_00183...</td>\n",
       "      <td>../data/val/val/indoors/scene_00019/scan_00183...</td>\n",
       "      <td>../data/val/val/indoors/scene_00019/scan_00183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>../data/val/val/indoors/scene_00020/scan_00184...</td>\n",
       "      <td>../data/val/val/indoors/scene_00020/scan_00184...</td>\n",
       "      <td>../data/val/val/indoors/scene_00020/scan_00184...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 image  \\\n",
       "234  ../data/val/val/indoors/scene_00021/scan_00188...   \n",
       "110  ../data/val/val/indoors/scene_00020/scan_00185...   \n",
       "248  ../data/val/val/indoors/scene_00021/scan_00189...   \n",
       "9    ../data/val/val/indoors/scene_00019/scan_00183...   \n",
       "93   ../data/val/val/indoors/scene_00020/scan_00184...   \n",
       "\n",
       "                                                 depth  \\\n",
       "234  ../data/val/val/indoors/scene_00021/scan_00188...   \n",
       "110  ../data/val/val/indoors/scene_00020/scan_00185...   \n",
       "248  ../data/val/val/indoors/scene_00021/scan_00189...   \n",
       "9    ../data/val/val/indoors/scene_00019/scan_00183...   \n",
       "93   ../data/val/val/indoors/scene_00020/scan_00184...   \n",
       "\n",
       "                                                  mask  \n",
       "234  ../data/val/val/indoors/scene_00021/scan_00188...  \n",
       "110  ../data/val/val/indoors/scene_00020/scan_00185...  \n",
       "248  ../data/val/val/indoors/scene_00021/scan_00189...  \n",
       "9    ../data/val/val/indoors/scene_00019/scan_00183...  \n",
       "93   ../data/val/val/indoors/scene_00020/scan_00184...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "def albu_transform_train():\n",
    "    return A.Compose([A.RandomRotate90(p=0.1),\n",
    "                      A.Transpose(p=0.5),\n",
    "                      A.Blur(p=0.01, blur_limit = 3),\n",
    "                      A.OneOf([A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5)],p=0.8)\n",
    "                      ],p=1)\n",
    "\n",
    "def albu_transform_valid():\n",
    "    return A.Compose([A.HorizontalFlip(p=0.5), A.RandomBrightnessContrast(p=0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, batch_size=6, dim=(768, 1024),transform = None , n_channels=3, shuffle=True):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.indices = self.data.index.to_list()\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.min_depth = 0.1\n",
    "        self.augmentation = transform\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if (index+1)*self.batch_size >len(self.indices):\n",
    "            self.batch_size = len(self.indices) - index * self.batch_size\n",
    "        # Generate one batch of data\n",
    "        # Generate indices of the batch\n",
    "        index = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        # Find list of IDs\n",
    "        batch = [self.indices[k] for k in index]\n",
    "        x, y = self.data_generation(batch)\n",
    "        return x, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def read_image(self,image_path):\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image,self.dim)\n",
    "        return image\n",
    "    \n",
    "    def load_depth_map(self,depth_map,mask):\n",
    "\n",
    "        depth_map = np.load(depth_map).squeeze()\n",
    "        mask = np.load(mask)\n",
    "        mask = mask>0\n",
    "        max_depth = min(300, np.percentile(depth_map, 99))\n",
    "        depth_map = np.clip(depth_map, self.min_depth, max_depth)\n",
    "        depth_map = np.log(depth_map, where=mask)\n",
    "\n",
    "        depth_map = np.ma.masked_where(~mask, depth_map)\n",
    "\n",
    "        depth_map = np.clip(depth_map, 0.1, np.log(max_depth))\n",
    "        depth_map = cv2.resize(depth_map, self.dim)\n",
    "        depth_map = np.expand_dims(depth_map, axis=2)\n",
    "        return depth_map\n",
    "\n",
    "\n",
    "    def load(self,image_path,depth_map,mask):\n",
    "        image = self.read_image(image_path)\n",
    "        image = tf.image.convert_image_dtype(image,tf.float32)\n",
    "        \n",
    "        depth_map = self.load_depth_map(depth_map,mask)\n",
    "        depth_map = tf.image.convert_image_dtype(depth_map, tf.float32)\n",
    "        \n",
    "        return image,depth_map\n",
    "    \n",
    "    def data_generation(self,batch):\n",
    "        x = np.empty((self.batch_size, *self.dim,self.n_channels))\n",
    "        y = np.empty((self.batch_size,  *self.dim , 1))\n",
    "        for i , batch_id in enumerate(batch):\n",
    "            if self.augmentation :\n",
    "                image_ = self.data[\"image\"][batch_id]\n",
    "                depth_ = self.data[\"depth\"][batch_id]\n",
    "                mask_ = self.data[\"mask\"][batch_id]\n",
    "                \n",
    "                image = self.read_image(image_)\n",
    "                depth = self.load_depth_map(depth_,mask_)\n",
    "                transformed = self.augmentation(image = image, mask = depth)\n",
    "\n",
    "                transformed_image = tf.image.convert_image_dtype(transformed['image'],tf.float32)\n",
    "                transformed_depth = tf.image.convert_image_dtype(transformed['mask'],tf.float32)\n",
    "                x[i,] , y[i,] = transformed_image,transformed_depth\n",
    "\n",
    "            else:\n",
    "                x[i,] , y[i,] = self.load(self.data[\"image\"][batch_id],\n",
    "                                        self.data[\"depth\"][batch_id],\n",
    "                                        self.data[\"mask\"][batch_id],\n",
    "                                        )\n",
    "        return x,y "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:58:48.603423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-20 14:58:48.787105: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "def display_image_mask(train_generator, num_images=4):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    count = 0\n",
    "    cmap = plt.cm.jet\n",
    "    for i in range(num_images):\n",
    "        Xtest, ytest = train_generator.__getitem__(i*np.random.randint(low=5,high=10))\n",
    "\n",
    "        output_img = ytest[0, :, :, 0]\n",
    "        if count >= num_images: break\n",
    "        count += 1\n",
    "        plt.subplot(2, 2, count)\n",
    "        plt.imshow(Xtest[0]) # 128x128x3\n",
    "        count += 1\n",
    "        plt.subplot(2, 2, count)\n",
    "        plt.imshow(output_img,cmap=cmap)\n",
    "\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "\n",
    "train_gen = DataGenerator(data=df, batch_size=6, dim=(HEIGHT, WIDTH))\n",
    "display_image_mask(train_gen,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(data=df, batch_size=6, dim=(HEIGHT, WIDTH),transform=albu_transform_train())\n",
    "display_image_mask(train_gen,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Point cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(DataGenerator(data=df, batch_size=6, dim=(HEIGHT, WIDTH))))\n",
    "print(samples[1][1].shape)\n",
    "print(samples[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visulaize_3d = False \n",
    "\n",
    "depth_vis = np.flipud(samples[1][1].squeeze())\n",
    "img_vis = np.flipud(samples[0][1].squeeze())\n",
    "\n",
    "print(depth_vis.shape,img_vis.shape)\n",
    "STEP = 3\n",
    "\n",
    "if visulaize_3d:    \n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "    for x in range(0, img_vis.shape[0], STEP):\n",
    "        for y in range(0, img_vis.shape[1], STEP):\n",
    "            ax.scatter(\n",
    "                [depth_vis[x, y]] * 3,\n",
    "                [y] * 3,\n",
    "                [x] * 3,\n",
    "                c=tuple(img_vis[x, y, :3] / 255),\n",
    "                s=3,\n",
    "            )\n",
    "        ax.view_init(45, 135)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, LeakyReLU, Concatenate \n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications import DenseNet169\n",
    "from tensorflow.keras.optimizers import Adam , SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpscaleBlock(Model):\n",
    "    def __init__(self,filters,name):\n",
    "        super(UpscaleBlock,self).__init__()\n",
    "        self.up = UpSampling2D(size=(2,2),interpolation='bilinear', name=name+'_upsampling2d')\n",
    "        self.concat = Concatenate(name=name+'_concat') \n",
    "        self.convA = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', name=name+'_convA')\n",
    "        self.reluA = LeakyReLU(alpha=0.2)\n",
    "        self.convB = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', name=name+'_convB')\n",
    "        self.reluB = LeakyReLU(alpha=0.2)\n",
    "\n",
    "    def call(self, x):        \n",
    "        b = self.reluB( self.convB( self.reluA( self.convA( self.concat( [self.up(x[0]), x[1]] ) ) ) ) )\n",
    "        return b \n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__() \n",
    "        self.base_model = DenseNet169(input_shape= (None,None,3),include_top = False,weights='imagenet')\n",
    "        print('Base model loaded {}'.format(DenseNet169.__name__))\n",
    "\n",
    "        # Create encoder model that produce final features along with multiple intermediate features\n",
    "        outputs = [self.base_model.outputs[-1]]\n",
    "        for name in ['pool1', 'pool2_pool', 'pool3_pool', 'conv1/relu'] : outputs.append( self.base_model.get_layer(name).output )        \n",
    "        self.encoder = Model(inputs=self.base_model.inputs, outputs=outputs)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "class Decoder(Model):\n",
    "    def __init__(self, decode_filters):\n",
    "        super(Decoder, self).__init__()        \n",
    "        self.conv2 =  Conv2D(filters=decode_filters, kernel_size=1, padding='same', name='conv2')        \n",
    "        self.up1 = UpscaleBlock(filters=decode_filters//2,  name='up1')\n",
    "        self.up2 = UpscaleBlock(filters=decode_filters//4,  name='up2')\n",
    "        self.up3 = UpscaleBlock(filters=decode_filters//8,  name='up3')\n",
    "        self.up4 = UpscaleBlock(filters=decode_filters//16, name='up4')        \n",
    "        self.conv3 = Conv2D(filters=1, kernel_size=3, strides=1, padding='same', name='conv3')       \n",
    "    \n",
    "    def call(self, features):        \n",
    "        x, pool1, pool2, pool3, conv1 = features[0], features[1], features[2], features[3], features[4]\n",
    "        up0 = self.conv2(x)        \n",
    "        up1 = self.up1([up0, pool3])        \n",
    "        up2 = self.up2([up1, pool2])        \n",
    "        up3 = self.up3([up2, pool1])        \n",
    "        up4 = self.up4([up3, conv1])        \n",
    "        return self.conv3( up4 )\n",
    "\n",
    "\n",
    "class DepthEstimate(Model):\n",
    "    def __init__(self):\n",
    "        super(DepthEstimate, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder( decode_filters = int(self.encoder.layers[-1].output[0].shape[-1] // 2 ) )\n",
    "        print('\\nModel created.')\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.decoder( self.encoder(x) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend  as K \n",
    "import tensorflow as tf\n",
    "\n",
    "def depth_loss_function(y_true, y_pred, theta=0.1, maxDepthVal=1000.0/10.0):\n",
    "    \n",
    "    # Point-wise depth\n",
    "    l_depth =K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "    \n",
    "    # Edges\n",
    "    dy_true, dx_true = tf.image.image_gradients(y_true)\n",
    "    dy_pred, dx_pred = tf.image.image_gradients(y_pred)\n",
    "    l_edges = K.mean(K.abs(dy_pred - dy_true) + K.abs(dx_pred - dx_true), axis=-1)\n",
    "\n",
    "    # Structural similarity (SSIM) index\n",
    "    l_ssim = K.clip((1 - tf.image.ssim(y_true, y_pred, maxDepthVal)) * 0.5, 0, 1)\n",
    "\n",
    "    # Weights\n",
    "    w1 = 1.0\n",
    "    w2 = 1.0\n",
    "    w3 = theta\n",
    "    # return (w1 * l_ssim) + (w3 * K.mean(l_depth)) + (w2 * K.mean(l_edges)) \n",
    "    return K.mean(l_depth)\n",
    "\n",
    "def scale_invarient_loss(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    computes scale invarient loss based on log of differences of there depth maps \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    log_diff = K.log(y_pred) - K.log(y_true)\n",
    "    num_pixels = tf.size(log_diff)\n",
    "    num_pixels = tf.cast(num_pixels,tf.float32)\n",
    "    \n",
    "    print(\"log diff \",log_diff)\n",
    "    if num_pixels == 0:\n",
    "        return  tf.constant(np.nan)\n",
    "    else :\n",
    "        loss = K.sqrt(K.sum(K.square(log_diff))/num_pixels - K.square(K.sum(log_diff))/K.square(num_pixels))    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "\n",
    "def create_model(existing='', is_twohundred=False, is_halffeatures=True,freeze_layer = False ):\n",
    "    n_layers = 100\n",
    "    if len(existing) == 0:\n",
    "        print('Loading base model (DenseNet)..')\n",
    "        # # Encoder Layers\n",
    "        # if is_twohundred:\n",
    "        #     base_model = applications.densenet.DenseNet201(input_shape=(None, None, 3), include_top=False)\n",
    "        # else:\n",
    "        base_model = DenseNet169(input_shape=(256, 256, 3), include_top=False)\n",
    "        print('Base model loaded.')\n",
    "\n",
    "        # Starting point for decoder\n",
    "        base_model_output_shape = base_model.layers[-1].output.shape\n",
    "\n",
    "        if not freeze_layer:\n",
    "            for layer in base_model.layers: \n",
    "                layer.trainable = True\n",
    "        else:\n",
    "            for layer in base_model.layers[n_layers]:\n",
    "                layer.trainable = False\n",
    "                \n",
    "        # Starting number of decoder filters\n",
    "        if is_halffeatures:\n",
    "            decode_filters = int(int(base_model_output_shape[-1])/2)\n",
    "        else:\n",
    "            decode_filters = int(base_model_output_shape[-1])\n",
    "\n",
    "        # Define upsampling layer\n",
    "        def upproject(tensor, filters, name, concat_with):\n",
    "            up_i = UpSampling2D((2, 2), name=name+'_upsampling2d')(tensor)\n",
    "            up_i = Concatenate(name=name+'_concat')([up_i, base_model.get_layer(concat_with).output]) # Skip connection\n",
    "            up_i = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', name=name+'_convA')(up_i)\n",
    "            up_i = LeakyReLU(alpha=0.2)(up_i)\n",
    "            up_i = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', name=name+'_convB')(up_i)\n",
    "            up_i = LeakyReLU(alpha=0.2)(up_i)\n",
    "            return up_i\n",
    "\n",
    "        # Decoder Layers\n",
    "        decoder = Conv2D(filters=decode_filters, kernel_size=1, padding='same', input_shape=base_model_output_shape, name='conv2')(base_model.output)\n",
    "\n",
    "        decoder = upproject(decoder, int(decode_filters/2), 'up1', concat_with='pool3_pool')\n",
    "        decoder = upproject(decoder, int(decode_filters/4), 'up2', concat_with='pool2_pool')\n",
    "        decoder = upproject(decoder, int(decode_filters/8), 'up3', concat_with='pool1')\n",
    "        decoder = upproject(decoder, int(decode_filters/16), 'up4', concat_with='conv1/relu')\n",
    "        decoder = upproject(decoder,int(decode_filters/32),'up5',concat_with='input_1')    \n",
    "        # if False: \n",
    "        # decoder = upproject(decoder, int(decode_filters/32), 'up5', concat_with='input_1')\n",
    "\n",
    "        # Extract depths (final layer)\n",
    "        conv3 = Conv2D(filters=1, kernel_size=3, strides=1, padding='same', name='conv3')(decoder)\n",
    "\n",
    "        # Create the model\n",
    "        model = Model(inputs=base_model.input, outputs=conv3)\n",
    "\n",
    "    print('Model created.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "depth_net = create_model(is_halffeatures=False)\n",
    "\n",
    "print(depth_net.input_shape)\n",
    "print(depth_net.output_shape)\n",
    "\n",
    "visualkeras.layered_view(depth_net, legend=True) # selected font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "# optimizer = SGD(momentum=0.9, nesterov=True)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "depth_net.compile(loss=scale_invarient_loss, optimizer=optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "path  = \"../data/training_2/\"\n",
    "checkpoint = ModelCheckpoint(os.path.join(path,\"depth_model.h5\"), monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train without Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(data=df, batch_size=4, dim=(HEIGHT, WIDTH),transform=None)\n",
    "EPOCHS = 5 \n",
    "\n",
    "History = depth_net.fit(\n",
    "    train_gen,\n",
    "    epochs=EPOCHS,  \n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), History.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(np.arange(0, N), History.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Loss on dataset\")\n",
    "plt.xlabel(\"Epoch \")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(data=df, batch_size=4, dim=(HEIGHT, WIDTH),transform=albu_transform_train())\n",
    "EPOCHS = 5 \n",
    "\n",
    "History = depth_net.fit(\n",
    "    train_gen,\n",
    "    epochs=EPOCHS,  \n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), History.history[\"loss\"], label=\"train_acc\")\n",
    "# plt.plot(np.arange(0, N), History.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Accuracy on augmented dataset \")\n",
    "plt.xlabel(\"Epoch \")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading weights \n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# custom_objects = {'BilinearUpSampling2D': UpSampling2D, 'depth_loss_function': depth_loss_function}\n",
    "model = load_model(\"../data/training_2/depth_model.h5\",custom_objects={'depth_loss_function': depth_loss_function})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = next(\n",
    "    iter(\n",
    "        DataGenerator(\n",
    "            data=df[265:].reset_index(drop=\"true\"), batch_size=6, dim=(HEIGHT, WIDTH)\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_depth_map(samples, model=None):\n",
    "    input, target = samples\n",
    "    cmap = plt.cm.jet\n",
    "    cmap.set_bad(color=\"black\")\n",
    "\n",
    "    pred = model.predict(input)\n",
    "    fig, ax = plt.subplots(6, 3, figsize=(50, 50))\n",
    "    for i in range(6):\n",
    "        ax[i, 0].imshow((input[i].squeeze()))\n",
    "        ax[i, 1].imshow((target[i].squeeze()), cmap=cmap)\n",
    "        ax[i, 2].imshow((pred[i].squeeze()), cmap=cmap)\n",
    "\n",
    "visualize_depth_map(test_loader,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydepth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
