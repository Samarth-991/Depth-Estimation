{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Total number of imgaes : 258\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from glob import glob \n",
    "import os.path as osp\n",
    "import numpy as np \n",
    "import torch as th \n",
    "from matplotlib import pyplot as plt \n",
    "import sys \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import random\n",
    "sys.path.append(\"../src\")\n",
    "from depth_model.model import PTModel\n",
    "from utils.load_tof_images import create_from_zip_absolute  as load_assignment_data\n",
    "from data_loader.data_loader_assignment import CreateAssignemntDataset\n",
    "from utils.data_transforms import pre_process\n",
    "from depth_model import inference as infer\n",
    "\n",
    "print(th.cuda.is_available())\n",
    "device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_checkpoint = \"../model/depth_model.ckpt\"\n",
    "\n",
    "path = \"../data/360_scans/\"\n",
    "child_rgb_files = glob(path+\"/*/rgb/*\")\n",
    "child_depth_files = glob(path+'/*/depth/*')\n",
    "print(\"Total number of imgaes :\",len(child_rgb_files))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and Visualize "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_multichannel(i):\n",
    "    if i.shape[2] == 3: return i\n",
    "    i = i[:,:,0]\n",
    "    return np.stack((i,i,i), axis=2)\n",
    "\n",
    "def tensor_to_numpy_image(image,rgb=True):\n",
    "    image = image.permute(1,0,2,3).squeeze(axis=0).cpu().detach().numpy()\n",
    "    if rgb:\n",
    "        image = image.squeeze(axis=1)\n",
    "        return np.array(image.transpose(1, 2, 0)*255, dtype=np.uint8) \n",
    "    else:\n",
    "        return image.squeeze(axis=0)\n",
    "\n",
    "def DepthNorm(x, maxDepth):\n",
    "    return maxDepth / x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_model = PTModel().float().to(device)\n",
    "checkpoint = th.load(model_checkpoint)\n",
    "depth_model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = CreateAssignemntDataset(child_rgb_files,child_depth_files,process_image=pre_process())\n",
    "\n",
    "random_idx = random.randint(0, len(data_loader)-1)\n",
    "sample = data_loader[random_idx]\n",
    "\n",
    "print(sample['image'].shape)\n",
    "print(sample['depth'].shape)\n",
    "\n",
    "plt.figure(figsize=(7,15))\n",
    "\n",
    "for i, sample_batch in enumerate(data_loader):\n",
    "    image = sample_batch['image'].to(device)\n",
    "    depth = sample_batch['depth'].to(device)\n",
    "    \n",
    "    y_pred = depth_model(sample_batch['image'].to(device))\n",
    "    y_pred = tensor_to_numpy_image(y_pred,rgb=False)\n",
    "    y_pred = DepthNorm(y_pred,0.05)*255\n",
    "    depth_image = tensor_to_numpy_image(depth,rgb=False)\n",
    "    resize_image =  tensor_to_numpy_image(image) # , shape, preserve_range=True, mode='reflect', anti_aliasing=True )\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(\"RGB Image\")\n",
    "    plt.imshow(resize_image)\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(\"Depth Image\")\n",
    "    plt.imshow(depth_image,cmap='gray')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Predicted Image\")\n",
    "    plt.imshow(y_pred,cmap='gray')  \n",
    "\n",
    "    # print(f\"Image resolution: {y_pred.shape}\")\n",
    "    # print(f\"Data type: {y_pred.dtype}\")\n",
    "    # print(f\"Min value: {np.min(y_pred)}\")\n",
    "    # print(f\"Max value: {np.max(y_pred)}\")\n",
    "    if i == 4:\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune  with Assignment Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save layer names\n",
    "layer_names = []\n",
    "for idx, (name, param) in enumerate(depth_model.named_parameters()):\n",
    "    layer_names.append(name)\n",
    "# Getting the depth layers for decoder \n",
    "# reverse layers\n",
    "layer_names.reverse()\n",
    "print(layer_names[0:20])\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the Model with Dataset provided by the Assignement with some changes "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune with LR \n",
    "The key idea is to gradually reduce the learning rate when going deeper into the network. The first layers should already have a pretty good understanding of general domain-agnostic patterns after pre-training. In a computer vision setting, the first layers may have learned to distinguish simple shapes and edges; in natural language processing, the first layers may be responsible for general word relationships. We don't want to update parameters on the first layers too much, so it makes sense to reduce the corresponding learning rates. In contrast, we would like to set a higher learning rate for the final layers, especially for the fully-connected classifier part of the network. Those layers usually focus on domain-specific information and need to be trained on new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune with Augmetations \n",
    "Adding Augmentations to generalize the Brightness and contrast in the image , though the environment seems restricted , but still it is a good practice to add randomize brightness and contrast. Adding Rotations and Gaussian blur is another augmentation technique that helped to improve the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as album\n",
    "\n",
    "def fine_tune(pretrined_model):\n",
    "    # save layer names\n",
    "    layer_names = []\n",
    "    for idx, (name, param) in enumerate(pretrined_model.named_parameters()):\n",
    "        layer_names.append(name)\n",
    "    # Getting the depth layers for decoder \n",
    "    # reverse layers\n",
    "    layer_names.reverse()\n",
    "\n",
    "    # learning rate\n",
    "    lr      = 1e-2\n",
    "    lr_mult = 0.9\n",
    "    # placeholder\n",
    "    parameters = []\n",
    "\n",
    "    # store params & learning rates\n",
    "    for idx, name in enumerate(layer_names):\n",
    "        # append layer parameters\n",
    "        parameters += [{'params': [p for n, p in pretrined_model.named_parameters() if n == name and p.requires_grad],\n",
    "                        'lr':     lr}]\n",
    "        # update learning rate\n",
    "        lr *= lr_mult\n",
    "    return parameters\n",
    "\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        album.OneOf(\n",
    "            [\n",
    "                album.RandomBrightnessContrast(p=1),\n",
    "                album.ChannelShuffle(p=1),\n",
    "                album.Sharpen(p=1),\n",
    "                album.SafeRotate(p=1)\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "\n",
    "fine_tune = False\n",
    "if fine_tune :\n",
    "    checkpoint = th.load(\"../model/depth_model.ckpt\") \n",
    "    depth_model.load_state_dict(checkpoint)\n",
    "    parameters  = fine_tune(pretrined_model=depth_model)\n",
    "    print(th.optim.Adam(parameters))\n",
    "\n",
    "## Run the train pipeline thereafter \n",
    "\n",
    "# Run the train.py with --pretrained argument from terminal \n",
    "# os.system(\"python train_assignment_data.py --path ../Data/Assignment_data/360_scans --pretrained ../model/child_depth_model.ckpt --model_name child_depth_model --outdir /content/model --epochs 40 \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference on Assignment images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_fpath = child_rgb_files[np.random.randint(0,255)]\n",
    "depth_fpath = rgb_fpath.replace('rgb','depth')\n",
    "calib_fpath = os.path.dirname(rgb_fpath).replace('rgb','calibration/0')\n",
    "\n",
    "data = load_assignment_data(rgb_fpath=rgb_fpath,depthmap_fpath=depth_fpath,calibration_fpath=calib_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "predicted_image  = infer.inference_rgbimage(data[8],depth_scale=data[4],depth_image_size=(480,640))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f76eff560b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"RGB Image\")\n",
    "plt.imshow(data[8])\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Depth Map Image\")\n",
    "plt.imshow(np.asarray(data[3],dtype=np.float32),cmap='gray')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"predicted Image\")\n",
    "plt.imshow(predicted_image,cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
